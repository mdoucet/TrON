import sys
import os
import json
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.lines as mlines

from matplotlib.path import Path
from matplotlib.patches import PathPatch

try:
    import bumps
    from bumps import dream
    HAS_BUMPS  = True
except:
    print("Summary_plot could not import bumps")
    HAS_BUMPS = False

if HAS_BUMPS:
    from . import fit_uncertainties, model_utils
    from refl1d.names import FitProblem
    from bumps.serialize import load_file
    from refl1d.bumps_interface import fitplugin


def load_problem(json_file: str) -> FitProblem:
    bumps.cli.install_plugin(fitplugin)
    return load_file(json_file)

def plot_sld(profile_file, label, show_cl=True, z_offset=0.0):
    """
        :param profile_file: File containing the SLD profile.
        :param label: Label for the plot.
        :param show_cl: Show the confidence limits.
        :param z_offset: Offset to apply to the z-axis when plotting.
    """
    if not os.path.isfile(profile_file):
        print("Could not find %s" % profile_file)
        return

    pre_sld = np.loadtxt(profile_file).T
    linewidth = 1 if show_cl else 2

    expt_file = profile_file.replace('-profile.dat', '-expt.json')
    if '-1-' in profile_file:
        profile_file = profile_file.replace('-1-', '-')

    if show_cl and HAS_BUMPS:
        # Sanity check
        mc_file = profile_file.replace('-profile.dat', '-chain.mc')
        if not os.path.isfile(mc_file):
            mc_file = profile_file.replace('-profile.dat', '-chain.mc.gz')
        if not os.path.isfile(mc_file):
            print("Could not find: %s" % mc_file)
            return

        # Load the model that was used for fitting
        expt = model_utils.expt_from_json_file(expt_file, set_ranges=True)
        problem = FitProblem(expt)
        model_path = profile_file.replace('-profile.dat', '')
        state = dream.state.load_state(model_path)

        z, best, low, high = fit_uncertainties.get_sld_contour(problem, state, cl=90, align=-1)[0]

        # Find the starting point of the distribution
        for i in range(len(best)-1, 0, -1):
            if np.fabs(best[i] - best[i-1]) > 0.001:
                break

        _z = z[i]-z+z_offset
        plt.plot(_z[:i], best[:i], markersize=4, label=label, linewidth=linewidth,)
        plt.fill_between(_z[:i], low[:i], high[:i], alpha=0.2, color=plt.gca().lines[-1].get_color())
    else:
        plt.plot(pre_sld[0][-1]-pre_sld[0]+z_offset, pre_sld[1], markersize=4,
                 label=label, linewidth=linewidth,)


def plot_dyn_data(dynamic_run, initial_state, final_state, first_index=0, last_index=-1,
                  dyn_data_dir=None, dyn_fit_dir=None, model_name='__model', scale=1):
    """
        Plot the dynamic data for a given run, and display the initial and final states.
    """
    # Fit results
    pre_fit = None
    if os.path.isfile(initial_state):
        pre_fit = np.loadtxt(initial_state).T

    post_fit = None
    if os.path.isfile(final_state):
        post_fit = np.loadtxt(final_state).T

    # Dynamic data
    _file_list = sorted(os.listdir(dyn_data_dir))
    fig, ax = plt.subplots(dpi=150, figsize=(5,8))
    plt.subplots_adjust(left=0.15, right=.95, top=0.98, bottom=0.1)

    if pre_fit is not None:
        idx = pre_fit[3]<pre_fit[2]
        plt.errorbar(pre_fit[0][idx], pre_fit[2][idx], yerr=pre_fit[3][idx], linewidth=1, 
                    markersize=2, marker='.', linestyle='',
                    color='darkgreen', label='Pre cycle 1')
        plt.plot(pre_fit[0], pre_fit[4], linewidth=1, markersize=2, marker='', color='black', zorder=400)

    # Get only the files for the run we're interested in
    _good_files = [_f for _f in _file_list if _f.startswith('r%d_t' % dynamic_run)]

    print(len(_good_files))

    scale = 1.
    multiplier = 10
    file_list = []

    # Check timing
    first_time = int(os.path.splitext(_good_files[first_index])[0].replace('r%d_t' % dynamic_run, ''))
    second_time = int(os.path.splitext(_good_files[first_index+1])[0].replace('r%d_t' % dynamic_run, ''))
    delta_t = second_time - first_time

    for _file in _good_files[first_index:last_index]:
        if _file.startswith('r%d_t' % dynamic_run):
            scale *= 1
            _data = np.loadtxt(os.path.join(dyn_data_dir, _file)).T
            _data_name, _ = os.path.splitext(_file)
            _time = int(_data_name.replace('r%d_t' % dynamic_run, ''))
            _label = '%d < t < %d s' % (_time, _time+delta_t)
 
            # Get fit if it exists
            fit_file = os.path.join(dyn_fit_dir, _data_name, '%s-refl.dat' % model_name)

            if os.path.isfile(fit_file):
                fit_data = np.loadtxt(fit_file).T
                plt.plot(fit_data[0], fit_data[4]*scale, markersize=2, marker='', linewidth=1, color='black')

            if len(_data)>1:
                idx = _data[2]<_data[1]
                plt.errorbar(_data[0][idx], _data[1][idx]*scale,
                             yerr=_data[2][idx]*scale, linewidth=1,
                             markersize=2, marker='.',  linestyle='', label=_label)

                scale *= multiplier
                file_list.append([_time, _data_name, _data_name])

    final_scale = scale/multiplier
    if post_fit is not None:
        idx = post_fit[3]<post_fit[2]
        plt.errorbar(post_fit[0][idx], post_fit[2][idx]*final_scale, yerr=post_fit[3][idx]*final_scale, linewidth=1, 
                    markersize=2, marker='.', linestyle='',
                    color='darkgreen', label='Post cycle 1')
        plt.plot(post_fit[0], post_fit[4]*final_scale, linewidth=1, color='darkblue')#, label='final')


    handles, labels = ax.get_legend_handles_labels()
    plt.legend(handles[::-1], labels[::-1], frameon=False, prop={'size': 7})
    plt.xlabel('Q ($1/\AA$)', fontsize=15)
    plt.ylabel('Reflectivity', fontsize=15)
    plt.yscale('log')
    plt.xscale('log')
    ax.yaxis.labelpad = 1

    plt.show()
    return file_list


def plot_dyn_sld(file_list, initial_state, final_state,
                 dyn_fit_dir=None, model_name='__model',
                 show_cl=True, legend_font_size=6,
                 max_z=None, reverse=True, sld_range=None,
                 initial_z_offset=0, final_z_offset=0
                ):

    fig, ax = plt.subplots(dpi=200, figsize=(5, 4.1))
    plt.subplots_adjust(left=0.15, right=.95, top=0.95, bottom=0.15)

    # Plot initial state
    if initial_state is not None:
        plot_sld(initial_state, 'Initial state', show_cl=False, z_offset=initial_z_offset)  

    _file_list = reversed(file_list) if reverse else file_list
    delta_t = int(file_list[1][0]) - int(file_list[0][0])

    for _file in _file_list:
        profile_file = os.path.join(dyn_fit_dir, str(_file[2]), '%s-profile.dat' % model_name)
        plot_sld(profile_file, '%d < t < %d s' % (int(_file[0]), int(_file[0])+delta_t),
                 show_cl=HAS_BUMPS and show_cl)
            
    # Plot final OCP
    if final_state is not None:
        plot_sld(final_state, 'Final state', show_cl=False, z_offset=final_z_offset)       
        
    handles, labels = ax.get_legend_handles_labels()
    plt.legend(handles[::-1], labels[::-1], loc='lower right', frameon=False, fontsize=legend_font_size)
    if max_z is not None:
        plt.xlim(-20, max_z)
    if sld_range is not None:
        plt.ylim(sld_range[0], sld_range[1])
    plt.xlabel('z ($\AA$)', fontsize=14)
    plt.ylabel('SLD ($10^{-6}/\AA^2$)', fontsize=14)
    plt.show()


def trend_data(file_list, initial_state, final_state, label='',
                 fit_dir=None, dyn_data_dir=None, dyn_fit_dir=None, model_name='__model',
                 model_file=None, newplot=True, plot_chi2=True, add_plot=0):
    """
        sei_thick.append(item['sei thickness'][which])
    sei_dthick.append(item['sei thickness']['std'])
    """
    # Get the varying parameters, which are assumed to be the same for all data sets
    par_file = os.path.join(dyn_fit_dir, str(file_list[0][2]), '%s.par' % model_name)
    if not os.path.isfile(par_file):
        par_file = os.path.join(dyn_fit_dir, str(file_list[-1][2]), '%s.par' % model_name)

    trend_data = dict()
    trend_err = dict()
    chi2 = []
    nlprior = []
    timestamp = []

    with open(par_file, 'r') as fd:
        for line in fd.readlines():
            par = ' '.join(line.split(' ')[0:2])
            if 'intensity' not in par:
                trend_data[par] = []
                trend_err[par] = []
                
    # Go through each file and retrieve the parameters
    # 'which' defines the value to select. It can either be 'mean' of 'best'.
    which = 'mean'
    for _file in file_list:
        err_file = os.path.join(dyn_fit_dir, str(_file[2]), '%s.err' % model_name)
        err_json = os.path.join(dyn_fit_dir, str(_file[2]), '%s-err.json' % model_name)
        bayes_json = os.path.join(dyn_fit_dir, str(_file[2]), '%s-bayes.dat' % model_name)

        if os.path.isfile(err_json):
            with open(bayes_json) as fd:
                for l in fd.readlines():
                    if l.startswith('NLL'):
                        toks = l.split(':')
                        _nll = float(toks[1])
                    if l.startswith('NLPrior'):
                        toks = l.split(':')
                        _nlprior = float(toks[1])
                    if l.startswith('Points'):
                        toks = l.split(':')
                        _npts = float(toks[1])
                chi2.append(_nll/_npts)
                nlprior.append(_nlprior/_npts)

            with open(err_json) as fd:
                m = json.load(fd)
                for par in trend_data.keys():
                    trend_data[par].append(m[par][which])
                    trend_err[par].append(m[par]['std'])

            timestamp.append(float(_file[0]))

    # Read initial and final states
    steady_values = dict()
    steady_err = dict()
    steady_times = dict()

    initial_file = os.path.join(fit_dir, str(initial_state), '__model-expt.json')
    if os.path.isfile(initial_file):
        with open(initial_file) as fd:
            m = json.load(fd)
            for par in trend_data.keys():
                for layer in m['sample']['layers']:
                    #print(layer)
                    rho = layer['material']['rho']
                    irho = layer['material']['irho']
                    thickness = layer['thickness']
                    interface = layer['interface']
                    for p in [rho, irho, thickness, interface]:
                        if p['name'] == par:
                            steady_values[par] = [p['value'], ]
                            steady_times[par] = [timestamp[0]-50, ]
                            print(par, p['value'])

    
    final_file = os.path.join(fit_dir, str(final_state), '__model-expt.json')
    if os.path.isfile(final_file):
        with open(final_file) as fd:
            m = json.load(fd)
            for par in trend_data.keys():
                for layer in m['sample']['layers']:
                    #print(layer)
                    rho = layer['material']['rho']
                    irho = layer['material']['irho']
                    thickness = layer['thickness']
                    interface = layer['interface']
                    for p in [rho, irho, thickness, interface]:
                        if p['name'] == par:
                            if p['name'] in steady_values:
                                steady_values[par].append(p['value'])
                                steady_times[par].append(timestamp[-1]+50)
                            else:
                                steady_values[par] = [p['value'], ]
                                steady_times[par] = [timestamp[-1]+50, ]
                        


    # Plot trend data
    n_tot = len(trend_data.keys()) + add_plot
    if plot_chi2:
        n_tot += 1

    if newplot:
        ysize = len(trend_data.keys()) * 2 + 6
        fig, axs = plt.subplots(n_tot,1, dpi=100, figsize=(6,ysize), sharex=True)
        plt.subplots_adjust(left=0.15, right=.95, top=0.98, bottom=0.1)

    n_current = 1
    for i, par in enumerate(trend_data.keys()):
        ax = plt.subplot(n_tot, 1, i+1)
        plt.errorbar(timestamp, trend_data[par], yerr=trend_err[par], label=par, marker='.', markersize=8, linestyle='--')
        #plt.xlabel('seconds')
        
        if par in steady_values:
            plt.plot(steady_times[par], steady_values[par], linestyle='', marker='*', markersize=10)
        
        plt.ylabel(par)
        #plt.legend(frameon=False)

    if plot_chi2:
        ax = plt.subplot(n_tot, 1, n_tot)
        plt.plot(timestamp, chi2, label='$\chi^2$')
        plt.plot(timestamp, nlprior, label='prior', linestyle='--')
        plt.ylabel('$\chi^2$')
        plt.legend(frameon=False)

    plt.xlabel("Time (seconds)")

    # Trend output
    trend_file = os.path.join(dyn_fit_dir, 'trend-%s.json' % model_name)
    with open(trend_file, 'w') as fp:
        print("Output saved to", trend_file)
        json.dump([timestamp, trend_data, trend_err, chi2], fp)

    return trend_data, trend_err, timestamp
        

def write_md_table(trend_data_file):
    """
        The trend data file is saved as:
            data[0] is the array of times
            data[1] is a dict of parameter values
            data[2] is the corresponding dict of uncertainties
    """
    with open(trend_data_file) as fd:
        data = json.load(fd)

        output_file = trend_data_file.replace('.json', '-table.md')
        with open(output_file, 'w') as output:
            # Write header
            headers = data[1].keys()
            header = '| Time | ' + '|'.join(headers) + '| chi2 |\n'
            header += '| ' + '|'.join((len(headers)+2)*['---']) + '|\n'
            output.write(header)

            for i in range(len(data[0])):
                entry = '| %g ' % (data[0][i])
                for k in data[1].keys():
                    entry += '| %4.2f ± %4.2f ' % (data[1][k][i], data[2][k][i])
                entry += '| %g |\n' % data[3][i]
                output.write(entry)


def detect_changes(dynamic_run, dyn_data_dir, first=0, last=-1, out_array=None):

    compiled_array = []
    compiled_times = []

    _file_list = sorted(os.listdir(dyn_data_dir))

    # Get only the files for the run we're interested in
    _good_files = [_f for _f in _file_list if _f.startswith('r%d_t' % dynamic_run)]

    print(len(_good_files))
    chi2 = []
    asym = []
    t = []
    skipped = 0
    previous = None
    previous_q = None
    previous = None
    previous_err = None
    
    min_q = 0.0154
    for _file in _good_files[first:last]:
        if _file.startswith('r%d_t' % dynamic_run):
            _data = np.loadtxt(os.path.join(dyn_data_dir, _file)).T
            if len(_data) == 0:
                continue
            idx = _data[0] >= min_q
            _data_name, _ = os.path.splitext(_file)
            _time = int(_data_name.replace('r%d_t' % dynamic_run, ''))
            compiled_array.append([_data[0][idx], _data[1][idx], _data[2][idx]])
            compiled_times.append(_time)

            if previous is not None:
                if len(_data[1]) == len(previous):
                    delta = np.mean((_data[1]-previous)**2/(_data[2]**2+previous_err**2))
                    chi2.append(delta)
                    _asym = np.mean((_data[1]-previous)/(_data[1]+previous))
                    asym.append(_asym)
                    t.append(_time)
                    
                elif True:
                    old_r = []
                    old_err = []
                    new_r = []
                    new_err = []
                    
                    for i, q in enumerate(_data[0]):
                        idx = np.argwhere(previous_q == q)
                        #print(idx)
                        if len(idx) > 0:
                            new_r.append(_data[1][i])
                            new_err.append(_data[2][i])
                            old_r.append(previous[idx[0][0]])
                            old_err.append(previous_err[idx[0][0]])

                    old_r = np.asarray(old_r)
                    old_err = np.asarray(old_err)
                    new_r = np.asarray(new_r)
                    new_err = np.asarray(new_err)

                    delta = np.mean((new_r - old_r)**2 / (new_err**2 + old_err**2))
                    #delta = np.mean((new_r - old_r)**2 / (new_err**2))
                    chi2.append(delta)
                    _asym = np.mean((new_r-old_r)/(new_r+old_r))
                    asym.append(_asym)
                    t.append(_time)
                
                previous_q = _data[0]
                previous = _data[1]
                previous_err = _data[2]

                        
                    #print("Unequal length: %s" % _file)
            else:
                print("Ref %s" % _file)
                previous_q = _data[0]
                previous = _data[1]
                previous_err = _data[2]

    if out_array:
        #np.save(out_array, np.asarray(compiled_array))
        #np.save(out_array+'_times', np.asarray(compiled_times))
        np.savetxt(out_array+'_chi2.txt', t)
        np.savetxt(out_array+'_times.txt', t)
    print("Skipped: %s" % skipped)
    fig = plt.figure(dpi=100, figsize=[8,4])
    plt.plot(t, chi2, markersize=10, marker='.', linestyle='--', label='$\chi^2$')
    #plt.plot(t, 10*np.asarray(asym), label='Asym [x10]')
    #plt.legend(frameon=False)
    plt.ylabel('$\chi^2$')
    plt.xlabel('Time (sec)')
    return t, chi2


def package_data(dynamic_run, dyn_data_dir, first=0, last=-1, qmin=0, qmax=1, max_len=None, out_array=None):

    compiled_array = []
    compiled_times = []
    data_array = []
    
    _file_list = sorted(os.listdir(dyn_data_dir))

    # Get only the files for the run we're interested in
    _good_files = [_f for _f in _file_list if _f.startswith('r%d_t' % dynamic_run)]

    print(len(_good_files))
    asym = []
    t = []
    skipped = 0
    previous = None
    
    min_q = qmin
    max_q = qmax

    for i, _file in enumerate(_good_files[first:last]):
        if _file.startswith('r%d_t' % dynamic_run):
            print(_file)
            _data = np.loadtxt(os.path.join(dyn_data_dir, _file)).T
            if np.min(_data[0]) > min_q:
                min_q = np.min(_data[0])
            if np.max(_data[0]) < max_q:
                max_q = np.max(_data[0])

            _data_name, _ = os.path.splitext(_file)
            _time = int(_data_name.replace('r%d_t' % dynamic_run, ''))
            data_array.append([_data_name, _time, _data])

    for i, _data in enumerate(data_array):
        idx = (_data[2][0] >= min_q) & (_data[2][0] < max_q)
        if max_len is not None and len(_data[2][0][idx]) > max_len:
            data2 = _data[2][0][idx]
            print(data2[-max_len:])
            compiled_array.append([data2[-max_len:], _data[2][1][idx][-max_len:], _data[2][2][idx][-max_len:]])
            compiled_times.append(_data[1])
        else:
            compiled_array.append([_data[2][0][idx], _data[2][1][idx], _data[2][2][idx]])
            compiled_times.append(_data[1])

    compiled_array = np.asarray(compiled_array)
    compiled_times = np.asarray(compiled_times)
    print(compiled_array.shape)
    print(np.max(compiled_array[0][0]))
    if out_array:
        np.save(out_array, compiled_array)
        np.save(out_array+'_times',compiled_times )
    return compiled_times, compiled_array

def package_json_data(dynamic_run, dyn_data_dir, out_array=None):

    compiled_array = []
    compiled_times = []
    data_array = []
    
    _file_list = sorted(os.listdir(dyn_data_dir))

    # Get only the files for the run we're interested in
    _good_files = [_f for _f in _file_list if _f.startswith('r%d_t' % dynamic_run)]

    for i, _file in enumerate(_good_files):
        if _file.startswith('r%d_t' % dynamic_run):
            
            _data = np.loadtxt(os.path.join(dyn_data_dir, _file)).T
            print(i, _file, len(_data[0]))
            _data_name, _ = os.path.splitext(_file)
            _time = int(_data_name.replace('r%d_t' % dynamic_run, ''))
            compiled_array.append(_data.tolist())
            compiled_times.append(_time)

    if out_array:
        with open(out_array, 'w') as fp:
            json.dump(dict(times=compiled_times, data=compiled_array), fp)
        
    return compiled_times, compiled_array

def main(dynamic_run, dyn_data_dir, model_file, initial_state, final_state, results_dir,
         first_item=0, last_item=-1):
    initial_refl = initial_state.replace('expt.json', 'refl.dat')
    final_refl = final_state.replace('expt.json', 'refl.dat')
    model_name = os.path.basename(model_file).replace('.py', '')

    # Generate plot of the reflectivity data
    plotted_data = plot_dyn_data(dynamic_run, initial_refl, final_refl,
                                 dyn_data_dir=dyn_data_dir, dyn_fit_dir=results_dir, model_name=model_name,
                                 first_index=first_item, last_index=last_item)
    plt.savefig(os.path.join(results_dir, 'dyn-%d.png' % dynamic_run))
    plt.savefig(os.path.join(results_dir, 'dyn-%d.svg' % dynamic_run))

    # Generate plot of the SLD profiles
    initial_sld = initial_state.replace('expt.json', 'profile.dat')
    final_sld = final_state.replace('expt.json', 'profile.dat')

    plot_dyn_sld(plotted_data, initial_sld, final_sld,
                 dyn_fit_dir=results_dir, 
                 show_cl=True, model_name=model_name, legend_font_size=8)
    plt.savefig(os.path.join(results_dir, 'sld-%d.png' % dynamic_run))
    plt.savefig(os.path.join(results_dir, 'sld-%d.svg' % dynamic_run))
